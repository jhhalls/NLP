{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "121d3e96",
   "metadata": {},
   "source": [
    "### 1. Problem Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5da65da",
   "metadata": {},
   "source": [
    "The problem given is __Fake News Detection using Natural Language Processing__.<br>\n",
    "Given Data has 4 features:<br>\n",
    "__title__ - Title for the news article<br>\n",
    "__author__ - author of the article<br>\n",
    "__text__ - Body of the article<br>\n",
    "__label__ - Whether the news provided in the article is fake or not fake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffa2cd8",
   "metadata": {},
   "source": [
    "### 2. Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5954ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "538e0c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53c1076f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title              author  \\\n",
       "0   0  House Dem Aide: We Didn’t Even See Comey’s Let...       Darrell Lucus   \n",
       "1   1  FLYNN: Hillary Clinton, Big Woman on Campus - ...     Daniel J. Flynn   \n",
       "2   2                  Why the Truth Might Get You Fired  Consortiumnews.com   \n",
       "3   3  15 Civilians Killed In Single US Airstrike Hav...     Jessica Purkiss   \n",
       "4   4  Iranian woman jailed for fictional unpublished...      Howard Portnoy   \n",
       "\n",
       "                                                text  label  \n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...      1  \n",
       "1  Ever get the feeling your life circles the rou...      0  \n",
       "2  Why the Truth Might Get You Fired October 29, ...      1  \n",
       "3  Videos 15 Civilians Killed In Single US Airstr...      1  \n",
       "4  Print \\nAn Iranian woman has been sentenced to...      1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b063d6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the column 'id'\n",
    "train_data.drop(columns={'id'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62a72572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20800, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3de0ef95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    10413\n",
       "0    10387\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19352f41",
   "metadata": {},
   "source": [
    "We can observe that the dataset is almost balanced as there are almost equal number of data points with label 0 and label 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec6d550",
   "metadata": {},
   "source": [
    "### 3. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d14104",
   "metadata": {},
   "source": [
    "#### 3.1. Checking for Duplicates and Dropping them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "204e47f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce16080b",
   "metadata": {},
   "source": [
    "There are 109 duplicate rows in train data and 6 duplicate rows in test data.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c6ba1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping duplicate rows\n",
    "train_data.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f9bb201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbe7967",
   "metadata": {},
   "source": [
    "#### 3.2. Checking for Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "095eac4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title      518\n",
       "author    1932\n",
       "text        39\n",
       "label        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "417fd24a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title     2.503504\n",
       "author    9.337393\n",
       "text      0.188488\n",
       "label     0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.isnull().sum()*100/train_data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba7c822",
   "metadata": {},
   "source": [
    "We can observe that there are many missing values. As there are many rows, we shouldn't drop all the rows. As 'author' is a categorical feature, we create a new category('missing') for missing authors. For missing values of title, text, we just replace NAN with ' '(space)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7114c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['title'].fillna(' ',inplace=True)\n",
    "train_data['text'].fillna(' ',inplace=True)\n",
    "train_data['author'].fillna('missing',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4acca77e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title     0\n",
       "author    0\n",
       "text      0\n",
       "label     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e1f8d4",
   "metadata": {},
   "source": [
    "### 4. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91646270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epanding English language contractions: https://stackoverflow.com/a/47091490/4084039\n",
    "import re\n",
    "\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"’\",\"'\",phrase)\n",
    "    phrase = re.sub(r\"”\",'\"',phrase)\n",
    "    phrase = re.sub(r\"“\",'\"',phrase)\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \"s\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b40620e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9953b10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\VS\n",
      "[nltk_data]     Chaitanya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f53f9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\VS\n",
      "[nltk_data]     Chaitanya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "341e7e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6289e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b73609",
   "metadata": {},
   "source": [
    "#### 4.1. title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c44314f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20691/20691 [00:02<00:00, 8518.16it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "preprocessed_titles = []\n",
    "# tqdm is for printing the status bar\n",
    "for sentance in tqdm(train_data['title'].values):\n",
    "    sent = decontracted(sentance)\n",
    "    sent=re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', sent) # remove hyperlinks\n",
    "    sent = re.sub('[^A-Za-z]+', ' ', sent) #remove spacial character, numbers: https://stackoverflow.com/a/5843547/4084039\n",
    "    sent = ' '.join(e for e in sent.split() if e not in stop_words) #removing stop words\n",
    "    sent=' '.join(lemmatizer.lemmatize(e) for e in sent.split()) #lemmatization\n",
    "    preprocessed_titles.append(sent.lower().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c048e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['title']=preprocessed_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dc69c6",
   "metadata": {},
   "source": [
    "#### 4.2. text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea025db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20691/20691 [00:55<00:00, 371.14it/s]\n"
     ]
    }
   ],
   "source": [
    "preprocessed_texts = []\n",
    "# tqdm is for printing the status bar\n",
    "for sentance in tqdm(train_data['text'].values):\n",
    "    sent = decontracted(sentance)\n",
    "    sent=re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', sent) # remove hyperlinks\n",
    "    sent = re.sub('[^A-Za-z]+', ' ', sent) #remove spacial characters, numbers: https://stackoverflow.com/a/5843547/4084039\n",
    "    sent = ' '.join(e for e in sent.split() if e not in stop_words) #removing stop words\n",
    "    sent=' '.join(lemmatizer.lemmatize(e) for e in sent.split()) #lemmatization\n",
    "    preprocessed_texts.append(sent.lower().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ada3d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['text']=preprocessed_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18951bc9",
   "metadata": {},
   "source": [
    "### 6. Splitting data into train, cv, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "62bb2ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fbb701d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output=train_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8ae180be",
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test,train_output,test_output=train_test_split(train_data.drop(columns={'label'}),output,test_size=0.3,stratify=output,random_state=0)\n",
    "train,cv,train_output,cv_output=train_test_split(train,train_output,test_size=0.3,stratify=train_output,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "782f7a4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10138, 3), (4345, 3), (6208, 3))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape,cv.shape,test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "72f91c42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10138,), (4345,), (6208,))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_output.shape,cv_output.shape,test_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ebedcc",
   "metadata": {},
   "source": [
    "### 6. Data Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d9ebe4",
   "metadata": {},
   "source": [
    "#### 6.1. title - TFIDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d947e4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "32d74261",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_tfidf_vectorizer = TfidfVectorizer(min_df=5)\n",
    "train_title_tfidf=title_tfidf_vectorizer.fit_transform(train['title'].values)\n",
    "cv_title_tfidf=title_tfidf_vectorizer.transform(cv['title'].values)\n",
    "test_title_tfidf=title_tfidf_vectorizer.transform(test['title'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cfdea0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the tfidf vectorizer\n",
    "import pickle\n",
    "with open(\"title_tfidf_vectorizer.pickle\",\"wb\") as fp:\n",
    "    pickle.dump(title_tfidf_vectorizer,fp,protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0d9c6ec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abandoned',\n",
       " 'abc',\n",
       " 'abedin',\n",
       " 'abedins',\n",
       " 'able',\n",
       " 'abortion',\n",
       " 'about',\n",
       " 'absolutely',\n",
       " 'abuse',\n",
       " 'abuses']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_tfidf_vectorizer.get_feature_names()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2dbf9b",
   "metadata": {},
   "source": [
    "#### 6.2. author - Response Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a53ff9e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2707,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['author'].unique().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c18cc8f",
   "metadata": {},
   "source": [
    "We can observe that there are 2707 authors in total train data. As dimensions will be high with one hot encoding, we use response coding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5e719a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_dict={}\n",
    "train_author=train.copy()\n",
    "train_author['label']=train_output\n",
    "train_author_1=train_author.groupby('author')\n",
    "for i in (train_author_1.groups):\n",
    "    group=train_author_1.get_group(i)\n",
    "    tot=group.shape[0]\n",
    "    fake=group[group['label']==1].shape[0]\n",
    "    prob_fake=fake/tot\n",
    "    prob_not_fake=1-prob_fake\n",
    "    prob_dict.update({i:[prob_not_fake,prob_fake]})\n",
    "\n",
    "keys=prob_dict.keys()\n",
    "\n",
    "train_author_response_code=[]\n",
    "for author in train['author']:\n",
    "    if author not in keys:\n",
    "        train_author_response_code.append([0.5,0.5])\n",
    "    else:\n",
    "        train_author_response_code.append(prob_dict.get(author))\n",
    "\n",
    "cv_author_response_code=[]\n",
    "for author in cv['author']:\n",
    "    if author not in keys:\n",
    "        cv_author_response_code.append([0.5,0.5])\n",
    "    else:\n",
    "        cv_author_response_code.append(prob_dict.get(author))\n",
    "        \n",
    "test_author_response_code=[]\n",
    "for author in test['author']:\n",
    "    if author not in keys:\n",
    "        test_author_response_code.append([0.5,0.5])\n",
    "    else:\n",
    "        test_author_response_code.append(prob_dict.get(author))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a283fb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the probability dictionary\n",
    "with open(\"prob_dict.pickle\",\"wb\") as fp:\n",
    "    pickle.dump(prob_dict,fp,protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389d083b",
   "metadata": {},
   "source": [
    "#### 6.3. text - TFIDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0b4b3366",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tfidf_vectorizer = TfidfVectorizer(min_df=10)\n",
    "train_text_tfidf=text_tfidf_vectorizer.fit_transform(train['text'].values)\n",
    "cv_text_tfidf=text_tfidf_vectorizer.transform(cv['text'].values)\n",
    "test_text_tfidf=text_tfidf_vectorizer.transform(test['text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "00d7a7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving text_tfidf_vectorizer\n",
    "with open(\"text_tfidf_vectorizer.pickle\",\"wb\") as fp:\n",
    "    pickle.dump(text_tfidf_vectorizer,fp,protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a38ac17e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aa',\n",
       " 'aaron',\n",
       " 'aaronkleinshow',\n",
       " 'ab',\n",
       " 'aba',\n",
       " 'aback',\n",
       " 'abadi',\n",
       " 'abandon',\n",
       " 'abandoned',\n",
       " 'abandoning']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tfidf_vectorizer.get_feature_names()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7189b45",
   "metadata": {},
   "source": [
    "#### 6.4. Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "53f03960",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6aa20002",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1166it [00:00, 10486.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1917494it [02:47, 11448.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. 1917494  words loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Reading glove vectors in python: https://stackoverflow.com/a/38230349/4084039\n",
    "#using pre-trained glove model\n",
    "import numpy as np\n",
    "def loadGloveModel(gloveFile):\n",
    "    print (\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r',errors = 'ignore',encoding=\"utf8\")\n",
    "    model = {}\n",
    "    for line in tqdm(f):\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.asarray([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print (\"Done.\",len(model),\" words loaded!\")\n",
    "    return model\n",
    "model = loadGloveModel(\"glove.42B.300d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c7433e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the glove model\n",
    "with open(\"model.pickle\",\"wb\") as fp:\n",
    "    pickle.dump(model,fp,protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fae6235e",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_words =  set(model.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed5d457",
   "metadata": {},
   "source": [
    "#### 6.4.1. title - Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2425e564",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10138/10138 [00:00<00:00, 15087.73it/s]\n",
      "100%|██████████| 4345/4345 [00:00<00:00, 15204.97it/s]\n",
      "100%|██████████| 6208/6208 [00:00<00:00, 14999.67it/s]\n"
     ]
    }
   ],
   "source": [
    "avg_w2v_vectors_title_train = []; # the avg-w2v for each title is stored in this list\n",
    "avg_w2v_vectors_title_cv = [];\n",
    "avg_w2v_vectors_title_test = [];\n",
    "\n",
    "for sentance in tqdm(train['title'].values): # for each title\n",
    "    vector = np.zeros(300) # as word vectors are of zero length\n",
    "    cnt_words =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sentance: # for each word in a title\n",
    "        if word in glove_words:\n",
    "            vector += model[word]\n",
    "            cnt_words += 1\n",
    "    if cnt_words != 0:\n",
    "        vector /= cnt_words\n",
    "    avg_w2v_vectors_title_train.append(vector)\n",
    "avg_w2v_vectors_title_train=np.array(avg_w2v_vectors_title_train)\n",
    "\n",
    "for sentance in tqdm(cv['title'].values): # for each title\n",
    "    vector = np.zeros(300) # as word vectors are of zero length\n",
    "    cnt_words =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sentance: # for each word in a title\n",
    "        if word in glove_words:\n",
    "            vector += model[word]\n",
    "            cnt_words += 1\n",
    "    if cnt_words != 0:\n",
    "        vector /= cnt_words\n",
    "    avg_w2v_vectors_title_cv.append(vector)\n",
    "avg_w2v_vectors_title_cv=np.array(avg_w2v_vectors_title_cv)\n",
    "\n",
    "for sentance in tqdm(test['title'].values): # for each title\n",
    "    vector = np.zeros(300) # as word vectors are of zero length\n",
    "    cnt_words =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sentance: # for each word in a title\n",
    "        if word in glove_words:\n",
    "            vector += model[word]\n",
    "            cnt_words += 1\n",
    "    if cnt_words != 0:\n",
    "        vector /= cnt_words\n",
    "    avg_w2v_vectors_title_test.append(vector)\n",
    "avg_w2v_vectors_title_test=np.array(avg_w2v_vectors_title_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cf8ce5",
   "metadata": {},
   "source": [
    "#### 6.4.2. text - Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "17084fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10138/10138 [00:30<00:00, 331.89it/s]\n",
      "100%|██████████| 4345/4345 [00:13<00:00, 332.09it/s]\n",
      "100%|██████████| 6208/6208 [00:18<00:00, 335.60it/s]\n"
     ]
    }
   ],
   "source": [
    "avg_w2v_vectors_text_train = []; # the avg-w2v for each text is stored in this list\n",
    "avg_w2v_vectors_text_cv = [];\n",
    "avg_w2v_vectors_text_test = [];\n",
    "\n",
    "for sentance in tqdm(train['text'].values): # for each text\n",
    "    vector = np.zeros(300) # as word vectors are of zero length\n",
    "    cnt_words =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sentance: # for each word in a text\n",
    "        if word in glove_words:\n",
    "            vector += model[word]\n",
    "            cnt_words += 1\n",
    "    if cnt_words != 0:\n",
    "        vector /= cnt_words\n",
    "    avg_w2v_vectors_text_train.append(vector)\n",
    "avg_w2v_vectors_text_train=np.array(avg_w2v_vectors_text_train)\n",
    "\n",
    "for sentance in tqdm(cv['text'].values): # for each text\n",
    "    vector = np.zeros(300) # as word vectors are of zero length\n",
    "    cnt_words =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sentance: # for each word in a text\n",
    "        if word in glove_words:\n",
    "            vector += model[word]\n",
    "            cnt_words += 1\n",
    "    if cnt_words != 0:\n",
    "        vector /= cnt_words\n",
    "    avg_w2v_vectors_text_cv.append(vector)\n",
    "avg_w2v_vectors_text_cv=np.array(avg_w2v_vectors_text_cv)\n",
    "\n",
    "for sentance in tqdm(test['text'].values): # for each text\n",
    "    vector = np.zeros(300) # as word vectors are of zero length\n",
    "    cnt_words =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sentance: # for each word in a text\n",
    "        if word in glove_words:\n",
    "            vector += model[word]\n",
    "            cnt_words += 1\n",
    "    if cnt_words != 0:\n",
    "        vector /= cnt_words\n",
    "    avg_w2v_vectors_text_test.append(vector)\n",
    "avg_w2v_vectors_text_test=np.array(avg_w2v_vectors_text_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17112d32",
   "metadata": {},
   "source": [
    "### 7. Combining all Encoded features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a3f993b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22b986f",
   "metadata": {},
   "source": [
    "#### 7.1. Combining TFIDF encoded features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0a4c779c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_final_tfidf=hstack((train_title_tfidf,train_author_response_code,train_text_tfidf))\n",
    "cv_data_final_tfidf=hstack((cv_title_tfidf,cv_author_response_code,cv_text_tfidf))\n",
    "test_data_final_tfidf=hstack((test_title_tfidf,test_author_response_code,test_text_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0578ce6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10138, 23423), (4345, 23423), (6208, 23423))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_final_tfidf.shape,cv_data_final_tfidf.shape,test_data_final_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde99b17",
   "metadata": {},
   "source": [
    "#### 7.2. Combining Word2Vec encoded features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "eb5d8c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_final_w2v=np.concatenate((avg_w2v_vectors_title_train,np.array(train_author_response_code),avg_w2v_vectors_text_train),axis=1)\n",
    "cv_data_final_w2v=np.concatenate((avg_w2v_vectors_title_cv,np.array(cv_author_response_code),avg_w2v_vectors_text_cv),axis=1)\n",
    "test_data_final_w2v=np.concatenate((avg_w2v_vectors_title_test,np.array(test_author_response_code),avg_w2v_vectors_text_test),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "aec1ac2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardizing the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler=StandardScaler()\n",
    "train_data_final_w2v=scaler.fit_transform(train_data_final_w2v)\n",
    "cv_data_final_w2v=scaler.transform(cv_data_final_w2v)\n",
    "test_data_final_w2v=scaler.transform(test_data_final_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c654f387",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the standard scaler\n",
    "with open(\"scaler.pickle\",\"wb\") as fp:\n",
    "    pickle.dump(scaler,fp,protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d6c749",
   "metadata": {},
   "source": [
    "### 8. Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9445e354",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score,f1_score,classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9645301e",
   "metadata": {},
   "source": [
    "#### 8.1. Multinomail Naive Bayes with TFIDF encoded features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ae73c188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For alpha=0.000010, train AUC=0.999995\n",
      "For alpha=0.000010, cv AUC=0.998262\n",
      "For alpha=0.000010, train f1 score=0.999010\n",
      "For alpha=0.000010, cv f1 score=0.979391\n",
      "--------------------------------------------------\n",
      "For alpha=0.000100, train AUC=0.999995\n",
      "For alpha=0.000100, cv AUC=0.998986\n",
      "For alpha=0.000100, train f1 score=0.999010\n",
      "For alpha=0.000100, cv f1 score=0.982948\n",
      "--------------------------------------------------\n",
      "For alpha=0.001000, train AUC=0.999994\n",
      "For alpha=0.001000, cv AUC=0.999408\n",
      "For alpha=0.001000, train f1 score=0.999010\n",
      "For alpha=0.001000, cv f1 score=0.986474\n",
      "--------------------------------------------------\n",
      "For alpha=0.010000, train AUC=0.999994\n",
      "For alpha=0.010000, cv AUC=0.999616\n",
      "For alpha=0.010000, train f1 score=0.999010\n",
      "For alpha=0.010000, cv f1 score=0.987867\n",
      "--------------------------------------------------\n",
      "For alpha=0.100000, train AUC=0.999993\n",
      "For alpha=0.100000, cv AUC=0.999662\n",
      "For alpha=0.100000, train f1 score=0.998911\n",
      "For alpha=0.100000, cv f1 score=0.990454\n",
      "--------------------------------------------------\n",
      "For alpha=1.000000, train AUC=0.999995\n",
      "For alpha=1.000000, cv AUC=0.999637\n",
      "For alpha=1.000000, train f1 score=0.999010\n",
      "For alpha=1.000000, cv f1 score=0.990685\n",
      "--------------------------------------------------\n",
      "For alpha=10.000000, train AUC=0.999999\n",
      "For alpha=10.000000, cv AUC=0.999461\n",
      "For alpha=10.000000, train f1 score=0.998812\n",
      "For alpha=10.000000, cv f1 score=0.978322\n",
      "--------------------------------------------------\n",
      "For alpha=100.000000, train AUC=0.999999\n",
      "For alpha=100.000000, cv AUC=0.999447\n",
      "For alpha=100.000000, train f1 score=0.998614\n",
      "For alpha=100.000000, cv f1 score=0.954173\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "alpha_range=[0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\n",
    "for i in alpha_range:\n",
    "    nb_clf=MultinomialNB(alpha=i, fit_prior=True)\n",
    "    nb_clf.fit(train_data_final_tfidf,train_output)\n",
    "    \n",
    "    train_prob=nb_clf.predict_proba(train_data_final_tfidf)[:,1]\n",
    "    train_AUC=roc_auc_score(train_output,train_prob)\n",
    "    print(\"For alpha=%f, train AUC=%f\" % (i,train_AUC))\n",
    "    \n",
    "    cv_prob=nb_clf.predict_proba(cv_data_final_tfidf)[:,1]\n",
    "    cv_AUC=roc_auc_score(cv_output,cv_prob)\n",
    "    print(\"For alpha=%f, cv AUC=%f\" % (i,cv_AUC))\n",
    "    \n",
    "    train_scores=nb_clf.predict(train_data_final_tfidf)\n",
    "    train_f1=f1_score(train_output,train_scores)\n",
    "    print(\"For alpha=%f, train f1 score=%f\" % (i,train_f1))\n",
    "    \n",
    "    cv_scores=nb_clf.predict(cv_data_final_tfidf)\n",
    "    cv_f1=f1_score(cv_output,cv_scores)\n",
    "    print(\"For alpha=%f, cv f1 score=%f\" % (i,cv_f1))\n",
    "\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ac8944",
   "metadata": {},
   "source": [
    "We can observe that with alpha=1, we got the best AUC and f1_score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8761a0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For alpha=0.1, test AUC=0.999408\n",
      "For alpha=0.1, test f1 score=0.989590\n"
     ]
    }
   ],
   "source": [
    "nb_clf_best=MultinomialNB(alpha=1, fit_prior=True)\n",
    "nb_clf_best.fit(train_data_final_tfidf,train_output)\n",
    "\n",
    "test_prob=nb_clf_best.predict_proba(test_data_final_tfidf)[:,1]\n",
    "test_AUC=roc_auc_score(test_output,test_prob)\n",
    "print(\"For alpha=0.1, test AUC=%f\" % (test_AUC))\n",
    "    \n",
    "test_scores=nb_clf_best.predict(test_data_final_tfidf)\n",
    "test_f1=f1_score(test_output,test_scores)\n",
    "print(\"For alpha=0.1, test f1 score=%f\" % (test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "349411ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Label 0       0.98      1.00      0.99      3116\n",
      "     Label 1       1.00      0.98      0.99      3092\n",
      "\n",
      "    accuracy                           0.99      6208\n",
      "   macro avg       0.99      0.99      0.99      6208\n",
      "weighted avg       0.99      0.99      0.99      6208\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_output,test_scores,target_names=['Label 0','Label 1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fc4d2c",
   "metadata": {},
   "source": [
    "#### 8.2. Logistic Regression with Word2Vec encoded features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fa7ddc89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For C=0.000100, train AUC=0.988264\n",
      "For C=0.000100, cv AUC=0.980405\n",
      "For C=0.000100, train f1 score=0.953616\n",
      "For C=0.000100, cv f1 score=0.929505\n",
      "--------------------------------------------------\n",
      "For C=0.001000, train AUC=0.999853\n",
      "For C=0.001000, cv AUC=0.996697\n",
      "For C=0.001000, train f1 score=0.997722\n",
      "For C=0.001000, cv f1 score=0.973856\n",
      "--------------------------------------------------\n",
      "For C=0.010000, train AUC=0.999894\n",
      "For C=0.010000, cv AUC=0.997323\n",
      "For C=0.010000, train f1 score=0.998714\n",
      "For C=0.010000, cv f1 score=0.975450\n",
      "--------------------------------------------------\n",
      "For C=0.100000, train AUC=0.999924\n",
      "For C=0.100000, cv AUC=0.995751\n",
      "For C=0.100000, train f1 score=0.998714\n",
      "For C=0.100000, cv f1 score=0.966313\n",
      "--------------------------------------------------\n",
      "For C=1.000000, train AUC=0.999920\n",
      "For C=1.000000, cv AUC=0.993544\n",
      "For C=1.000000, train f1 score=0.999010\n",
      "For C=1.000000, cv f1 score=0.955160\n",
      "--------------------------------------------------\n",
      "For C=10.000000, train AUC=0.999920\n",
      "For C=10.000000, cv AUC=0.992836\n",
      "For C=10.000000, train f1 score=0.999010\n",
      "For C=10.000000, cv f1 score=0.951405\n",
      "--------------------------------------------------\n",
      "For C=100.000000, train AUC=0.999919\n",
      "For C=100.000000, cv AUC=0.992698\n",
      "For C=100.000000, train f1 score=0.999010\n",
      "For C=100.000000, cv f1 score=0.950405\n",
      "--------------------------------------------------\n",
      "For C=1000.000000, train AUC=0.999919\n",
      "For C=1000.000000, cv AUC=0.992679\n",
      "For C=1000.000000, train f1 score=0.999010\n",
      "For C=1000.000000, cv f1 score=0.950405\n",
      "--------------------------------------------------\n",
      "For C=10000.000000, train AUC=0.999919\n",
      "For C=10000.000000, cv AUC=0.992677\n",
      "For C=10000.000000, train f1 score=0.999010\n",
      "For C=10000.000000, cv f1 score=0.950405\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "c_range=[0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]\n",
    "for i in c_range:\n",
    "    logistic_clf=LogisticRegression(C=i,max_iter=300)\n",
    "    logistic_clf.fit(train_data_final_w2v,train_output)\n",
    "    \n",
    "    train_prob=logistic_clf.predict_proba(train_data_final_w2v)[:,1]\n",
    "    train_AUC=roc_auc_score(train_output,train_prob)\n",
    "    print(\"For C=%f, train AUC=%f\" % (i,train_AUC))\n",
    "    \n",
    "    cv_prob=logistic_clf.predict_proba(cv_data_final_w2v)[:,1]\n",
    "    cv_AUC=roc_auc_score(cv_output,cv_prob)\n",
    "    print(\"For C=%f, cv AUC=%f\" % (i,cv_AUC))\n",
    "    \n",
    "    train_scores=logistic_clf.predict(train_data_final_w2v)\n",
    "    train_f1=f1_score(train_output,train_scores)\n",
    "    print(\"For C=%f, train f1 score=%f\" % (i,train_f1))\n",
    "    \n",
    "    cv_scores=logistic_clf.predict(cv_data_final_w2v)\n",
    "    cv_f1=f1_score(cv_output,cv_scores)\n",
    "    print(\"For C=%f, cv f1 score=%f\" % (i,cv_f1))\n",
    "\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9bf4f6",
   "metadata": {},
   "source": [
    "We can observe that with c=0.01, we got good AUC, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d7487231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For C=0.01, test AUC=0.996436\n",
      "For C=0.01, test f1 score=0.970540\n"
     ]
    }
   ],
   "source": [
    "logistic_clf_best=LogisticRegression(C=0.01,max_iter=300)\n",
    "logistic_clf_best.fit(train_data_final_w2v,train_output)\n",
    "\n",
    "test_prob=logistic_clf_best.predict_proba(test_data_final_w2v)[:,1]\n",
    "test_AUC=roc_auc_score(test_output,test_prob)\n",
    "print(\"For C=0.01, test AUC=%f\" % (test_AUC))\n",
    "    \n",
    "test_scores=logistic_clf_best.predict(test_data_final_w2v)\n",
    "test_f1=f1_score(test_output,test_scores)\n",
    "print(\"For C=0.01, test f1 score=%f\" % (test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f6350162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Label 0       0.96      0.98      0.97      3116\n",
      "     Label 1       0.98      0.96      0.97      3092\n",
      "\n",
      "    accuracy                           0.97      6208\n",
      "   macro avg       0.97      0.97      0.97      6208\n",
      "weighted avg       0.97      0.97      0.97      6208\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_output,test_scores,target_names=['Label 0','Label 1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e2ad6f",
   "metadata": {},
   "source": [
    "### 9. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb092b5",
   "metadata": {},
   "source": [
    "After observing the classification reports of the above two models, we can say that MultiNomial Naive Bayes Model provided best results on test data.\n",
    "\n",
    "__test AUC=0.999408__<br>\n",
    "__test f1 score=0.98959__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "74ee1ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the best model\n",
    "with open(\"nb_clf_best.pickle\",\"wb\") as fp:\n",
    "    pickle.dump(nb_clf_best,fp,protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
