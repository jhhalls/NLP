{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87341162",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\VS\n",
      "[nltk_data]     Chaitanya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\VS\n",
      "[nltk_data]     Chaitanya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "from scipy.sparse import hstack\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stop_words=stopwords.words('english')\n",
    "lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c45afa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"’\",\"'\",phrase)\n",
    "    phrase = re.sub(r\"”\",'\"',phrase)\n",
    "    phrase = re.sub(r\"“\",'\"',phrase)\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \"s\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dda6ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final(file):\n",
    "    print(\"Reading Data...\")\n",
    "    data=pd.read_csv(file)\n",
    "    \n",
    "    #dropping the column 'id'\n",
    "    data.drop(columns={'id'},inplace=True)\n",
    "    \n",
    "    print(\"Data Cleaning...\")\n",
    "    #dropping duplicate rows\n",
    "    data.drop_duplicates(inplace=True)\n",
    "    \n",
    "    data['title'].fillna(' ',inplace=True)\n",
    "    data['text'].fillna(' ',inplace=True)\n",
    "    data['author'].fillna('missing',inplace=True)\n",
    "    \n",
    "    print(\"Data Preprocessing...\")\n",
    "    preprocessed_titles = []\n",
    "    for sentance in (data['title'].values):\n",
    "        sent = decontracted(sentance)\n",
    "        sent=re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', sent) # remove hyperlinks\n",
    "        sent = re.sub('[^A-Za-z]+', ' ', sent) #remove spacial character, numbers: https://stackoverflow.com/a/5843547/4084039\n",
    "        sent = ' '.join(e for e in sent.split() if e not in stop_words) #removing stop words\n",
    "        sent=' '.join(lemmatizer.lemmatize(e) for e in sent.split()) #lemmatization\n",
    "        preprocessed_titles.append(sent.lower().strip())\n",
    "    data['title']=preprocessed_titles\n",
    "    \n",
    "    preprocessed_texts = []\n",
    "    for sentance in (data['text'].values):\n",
    "        sent = decontracted(sentance)\n",
    "        sent=re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', sent) # remove hyperlinks\n",
    "        sent = re.sub('[^A-Za-z]+', ' ', sent) #remove spacial characters, numbers: https://stackoverflow.com/a/5843547/4084039\n",
    "        sent = ' '.join(e for e in sent.split() if e not in stop_words) #removing stop words\n",
    "        sent=' '.join(lemmatizer.lemmatize(e) for e in sent.split()) #lemmatization\n",
    "        preprocessed_texts.append(sent.lower().strip())\n",
    "    data['text']=preprocessed_texts\n",
    "    \n",
    "    print(\"Data Encoding...\")\n",
    "    with open(\"title_tfidf_vectorizer.pickle\",\"rb\") as fp:\n",
    "        title_tfidf_vectorizer=pickle.load(fp)\n",
    "    title_tfidf=title_tfidf_vectorizer.transform(data['title'].values)\n",
    "    \n",
    "    with open(\"prob_dict.pickle\",\"rb\") as fp:\n",
    "        prob_dict=pickle.load(fp)\n",
    "    \n",
    "    keys=prob_dict.keys()\n",
    "    author_response_code=[]\n",
    "    for author in data['author']:\n",
    "        if author not in keys:\n",
    "            author_response_code.append([0.5,0.5])\n",
    "        else:\n",
    "            author_response_code.append(prob_dict.get(author))\n",
    "    \n",
    "    with open(\"text_tfidf_vectorizer.pickle\",\"rb\") as fp:\n",
    "        text_tfidf_vectorizer=pickle.load(fp)\n",
    "    text_tfidf=text_tfidf_vectorizer.transform(data['text'].values)\n",
    "    \n",
    "    data_final_tfidf=hstack((title_tfidf,author_response_code,text_tfidf))\n",
    "    \n",
    "    print(\"Loading best model and predicting the output labels...\")\n",
    "    with open(\"nb_clf_best.pickle\",\"rb\") as fp:\n",
    "        nb_clf_best=pickle.load(fp)\n",
    "    scores=nb_clf_best.predict(data_final_tfidf)\n",
    "    print(\"Done.\")\n",
    "    \n",
    "    print(\"Storing predicted labels in labels column...\")\n",
    "    data_with_labels=data.copy()\n",
    "    data_with_labels['label']=scores\n",
    "    data_with_labels.to_csv(\"test_data_with_predicted_labels.csv\",index=False)\n",
    "    print(\"Check 'test_data_with_predicted_labels.csv' for output.\")\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4203383c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Data...\n",
      "Data Cleaning...\n",
      "Data Preprocessing...\n",
      "Data Encoding...\n",
      "Loading best model and predicting the output labels...\n",
      "Done.\n",
      "Storing predicted labels in labels column...\n",
      "Check 'test_data_with_predicted_labels.csv' for output.\n",
      "[0 1 1 0 1 1 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "file=\"test.csv\"\n",
    "predicted_labels=final(file)\n",
    "print(predicted_labels[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-gpu",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
